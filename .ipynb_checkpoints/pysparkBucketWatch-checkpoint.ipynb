{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting telegram-send\n",
      "  Using cached telegram_send-0.25-py2.py3-none-any.whl (24 kB)\n",
      "Collecting appdirs\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting colorama\n",
      "  Using cached colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting python-telegram-bot>=12.1.1\n",
      "  Using cached python_telegram_bot-13.7-py3-none-any.whl (490 kB)\n",
      "Collecting tornado>=6.1\n",
      "  Using cached tornado-6.1-cp37-cp37m-manylinux2010_x86_64.whl (428 kB)\n",
      "Collecting APScheduler==3.6.3\n",
      "  Using cached APScheduler-3.6.3-py2.py3-none-any.whl (58 kB)\n",
      "Collecting pytz>=2018.6\n",
      "  Using cached pytz-2021.1-py2.py3-none-any.whl (510 kB)\n",
      "Collecting certifi\n",
      "  Using cached certifi-2021.5.30-py2.py3-none-any.whl (145 kB)\n",
      "Collecting cachetools==4.2.2\n",
      "  Using cached cachetools-4.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting tzlocal>=1.2\n",
      "  Using cached tzlocal-2.1-py2.py3-none-any.whl (16 kB)\n",
      "Collecting setuptools>=0.7\n",
      "  Using cached setuptools-57.4.0-py3-none-any.whl (819 kB)\n",
      "Collecting six>=1.4.0\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "\u001b[31mERROR: conda 4.8.2 requires ruamel_yaml>=0.11.14, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow 2.1.0 has requirement scipy==1.4.1; python_version >= \"3\", but you'll have scipy 1.5.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: appdirs, colorama, tornado, pytz, tzlocal, setuptools, six, APScheduler, certifi, cachetools, python-telegram-bot, telegram-send\n",
      "Successfully installed APScheduler-3.6.3 appdirs-1.4.4 cachetools-4.2.2 certifi-2021.5.30 colorama-0.4.4 python-telegram-bot-13.7 pytz-2021.1 setuptools-57.4.0 six-1.16.0 telegram-send-0.25 tornado-6.1 tzlocal-2.1\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/setuptools-57.4.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/_distutils_hack already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/appdirs.py already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/version.py already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/certifi-2021.5.30.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/tornado-6.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/colorama-0.4.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/certifi already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/cachetools already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/tzlocal already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/telegram_send.py already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/tzlocal-2.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/apscheduler already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/setuptools already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/telegram already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/__pycache__ already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/six.py already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/colorama already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/six-1.16.0.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/pkg_resources already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/cachetools-4.2.2.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/telegram_send-0.25.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/distutils-precedence.pth already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/pytz already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/APScheduler-3.6.3.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/pytz-2021.1.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/python_telegram_bot-13.7.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/tornado already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/appdirs-1.4.4.dist-info already exists. Specify --upgrade to force replacement.\u001b[0m\n",
      "\u001b[33mWARNING: Target directory /home/spark/shared/user-libs/python3.7/bin already exists. Specify --upgrade to force replacement.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install telegram-send"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def telegram_bot_sendtext(bot_message):\n",
    "    \n",
    "    bot_token = ''\n",
    "    bot_chatID = ''\n",
    "    send_text = 'https://api.telegram.org/bot' + bot_token + '/sendMessage?chat_id=' + bot_chatID + '&parse_mode=Markdown&text=' + bot_message\n",
    "\n",
    "    response = requests.get(send_text)\n",
    "\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ibmos2spark, os\n",
    "# @hidden_cell\n",
    "\n",
    "if os.environ.get('RUNTIME_ENV_LOCATION_TYPE') == 'external':\n",
    "    endpoint_6337e1afa96b41479fda20829220b743 = 'https://s3.us.cloud-object-storage.appdomain.cloud'\n",
    "else:\n",
    "    endpoint_6337e1afa96b41479fda20829220b743 = 'https://s3.private.us.cloud-object-storage.appdomain.cloud'\n",
    "\n",
    "credentials = {\n",
    "    'endpoint': endpoint_6337e1afa96b41479fda20829220b743,\n",
    "    'service_id': '',\n",
    "    'iam_service_endpoint': 'https://iam.cloud.ibm.com/oidc/token',\n",
    "    'api_key': '',\n",
    "    'access_key': '',\n",
    "    'secret_key': ''\n",
    "}\n",
    "\n",
    "configuration_name = 'spark_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType() \\\n",
    "          .add(\"qid\", IntegerType(), True) \\\n",
    "          .add(\"question\", StringType(), True) \\\n",
    "          .add(\"vots\", IntegerType(), True) \\\n",
    "          .add(\"answers\", IntegerType(), True) \\\n",
    "          .add(\"views\",  IntegerType(), True) \\\n",
    "          .add(\"tags\", StringType(), True) \\\n",
    "          .add(\"created_at\", TimestampType(), True) \\\n",
    "          .add(\"last_modifyed\", TimestampType(), True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# faz streaming com os arquivos que entrarem no bucket\n",
    "df = sparkSession.readStream.format('csv')\\\n",
    ".schema(schema)\\\n",
    ".option(\"header\",True)\\\n",
    ".option('sep', ',')\\\n",
    ".option(\"maxFilesPerTrigger\",1)\\\n",
    ".load(cos.url(obj, bucket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apenas le os arquivos no bucket para dentro de um dataframe\n",
    "bucket = ''\n",
    "obj = '*.csv'\n",
    "df = spark.read.options(delimiter=';',header=False).schema(schema).csv(cos.url(obj, bucket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import re\n",
    "word = \"['php', 'codeigniter']\"\n",
    "my_str = word[1:-1]\n",
    "my_str = re.sub(',',  '',    my_str)\n",
    "my_str = re.sub(\"'\",  '', my_str)\n",
    "print(my_str.split(\" \"))\n",
    "print(','.join(my_str.split(\" \")))\n",
    "print(type(my_str.split(\" \")))\n",
    "print(np.array(my_str.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "import re\n",
    "def strArray_to_str(my_str):\n",
    "    my_str = my_str[1:-1]\n",
    "    my_str = re.sub(',',  '',    my_str)\n",
    "    my_str = re.sub(\"'\",  '', my_str)   \n",
    "    #return my_str.split(\" \")\n",
    "    return ','.join(my_str.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_to_arr_udf = udf(strArray_to_str,StringType())\n",
    "df = df.withColumn(\"tags\",str_to_arr_udf(df[\"tags\"]))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import explode\n",
    "df.select(explode(df.tags)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save in a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "\n",
    "# This connection object is used to access your data and contains your credentials or project token.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "sparkSession = SparkSession(spark).builder.getOrCreate()\n",
    "\n",
    "db2_connection_url = 'jdbc:db2://{}:{}/{}:sslConnection=true;'.format(\n",
    "    f'{}.databases.appdomain.cloud',\n",
    "    32733,\n",
    "    'bludb'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = ''\n",
    "password = \"\"\"\"\"\"\n",
    "prop = {\"user\":user, \"password\":password}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.jdbc(url=db2_connection_url , table='\"BCZ79269\".\"SO_QUESTIONS\"', mode=\"append\",properties=prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### monitor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if questions.isStreaming == True:\n",
    "    message = \"ðŸš€ Scraping\\n\\nO observador estÃ¡ de pÃ©\"\n",
    "    telegram_bot_sendtext(message)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 with Spark",
   "language": "python3",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
