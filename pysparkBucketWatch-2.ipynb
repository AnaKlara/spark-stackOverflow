{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install telegram-send"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def telegram_bot_sendtext(bot_message):\n",
    "    \n",
    "    bot_token = ''\n",
    "    bot_chatID = ''\n",
    "    send_text = 'https://api.telegram.org/bot' + bot_token + '/sendMessage?chat_id=' + bot_chatID + '&parse_mode=Markdown&text=' + bot_message\n",
    "\n",
    "    response = requests.get(send_text)\n",
    "\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ibmos2spark, os\n",
    "# @hidden_cell\n",
    "\n",
    "if os.environ.get('RUNTIME_ENV_LOCATION_TYPE') == 'external':\n",
    "    endpoint_6337e1afa96b41479fda20829220b743 = 'https://s3.us.cloud-object-storage.appdomain.cloud'\n",
    "else:\n",
    "    endpoint_6337e1afa96b41479fda20829220b743 = 'https://s3.private.us.cloud-object-storage.appdomain.cloud'\n",
    "\n",
    "credentials = {\n",
    "    'endpoint': endpoint_6337e1afa96b41479fda20829220b743,\n",
    "    'service_id': '',\n",
    "    'iam_service_endpoint': 'https://iam.cloud.ibm.com/oidc/token',\n",
    "    'api_key': '',\n",
    "    'access_key': '',\n",
    "    'secret_key': ''\n",
    "}\n",
    "\n",
    "configuration_name = 'spark_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType() \\\n",
    "          .add(\"qid\", IntegerType(), True) \\\n",
    "          .add(\"question\", StringType(), True) \\\n",
    "          .add(\"vots\", IntegerType(), True) \\\n",
    "          .add(\"answers\", IntegerType(), True) \\\n",
    "          .add(\"views\",  IntegerType(), True) \\\n",
    "          .add(\"tags\", StringType(), True) \\\n",
    "          .add(\"created_at\", TimestampType(), True) \\\n",
    "          .add(\"last_modifyed\", TimestampType(), True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# apenas le os arquivos no bucket para dentro de um dataframe\n",
    "bucket = 'sparkbucketwatcheripynb-donotdelete-pr-3je2emjupdkz5l'\n",
    "obj = '*.csv'\n",
    "df = spark.read.options(delimiter=';',header=False).schema(schema).csv(cos.url(obj, bucket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "import re\n",
    "def strArray_to_str(my_str):\n",
    "    my_str = my_str[1:-1]\n",
    "    my_str = re.sub(',',  '',    my_str)\n",
    "    my_str = re.sub(\"'\",  '', my_str)   \n",
    "    #return my_str.split(\" \")\n",
    "    return ','.join(my_str.split(\" \"))\n",
    "str_to_arr_udf = udf(strArray_to_str,StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save in a database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "\n",
    "# This connection object is used to access your data and contains your credentials or project token.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "sparkSession = SparkSession(spark).builder.getOrCreate()\n",
    "\n",
    "db2_connection_url = 'jdbc:db2://{}:{}/{}:sslConnection=true;'.format(\n",
    "    f'{}.databases.appdomain.cloud',\n",
    "    32733,\n",
    "    'bludb'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = ''\n",
    "password = \"\"\"\"\"\"\n",
    "prop = {\"user\":user, \"password\":password}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### monitor "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if questions.isStreaming == True:\n",
    "    message = \"ðŸš€ Scraping\\n\\nO observador estÃ¡ de pÃ©\"\n",
    "    telegram_bot_sendtext(message)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = df.withColumn(\"tags\",str_to_arr_udf(df[\"tags\"]))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.write.jdbc(url=db2_connection_url , table='\"BCZ79269\".\"SO_QUESTIONS\"', mode=\"append\",properties=prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executeDF(df, epochId):\n",
    "    #df = df.withColumn(\"tags\",str_to_arr_udf(df[\"tags\"]))\n",
    "    df.write.jdbc(url=db2_connection_url , table='\"BCZ79269\".\"SO_QUESTIONS\"', mode=\"overwrite\",properties=prop)\n",
    "    message = \"ðŸš€ Scraping\\n\\nO observador estÃ¡ de pÃ©\"\n",
    "    telegram_bot_sendtext(message)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# faz streaming com os arquivos que entrarem no bucket\n",
    "df = sparkSession.readStream.format('csv')\\\n",
    ".schema(schema)\\\n",
    ".option(\"header\",True)\\\n",
    ".option('sep', ',')\\\n",
    ".option(\"maxFilesPerTrigger\",1)\\\n",
    ".load(cos.url(obj, bucket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apenas le os arquivos no bucket para dentro de um dataframe\n",
    "bucket = 'sparkbucketwatcheripynb-donotdelete-pr-3je2emjupdkz5l'\n",
    "obj = '*.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "  sparkSession.readStream.format(\"csv\").schema(schema).option('sep', ';').load(cos.url(obj, bucket))\n",
    "    .writeStream\n",
    "    .foreachBatch(executeDF)\n",
    "    .outputMode(\"update\")\n",
    "    .start()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 with Spark",
   "language": "python3",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
